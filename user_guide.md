# Kinetica SQLAlchemy Dialect User Guide

A comprehensive guide for using the Kinetica SQLAlchemy dialect to interact with Kinetica database.

## Table of Contents

1. [Installation and Setup](#installation-and-setup)
2. [Connection Configuration](#connection-configuration)
3. [Data Types](#data-types)
4. [Table Operations](#table-operations)
   - [Creating Tables](#creating-tables)
   - [Table Options and Properties](#table-options-and-properties)
   - [External Tables](#external-tables)
   - [CREATE TABLE AS](#create-table-as)
5. [Insert Operations](#insert-operations)
   - [Basic Insert](#basic-insert)
   - [Batch Insert with ki_insert](#batch-insert-with-ki_insert)
   - [Insert with Hints](#insert-with-hints)
   - [Insert from Select](#insert-from-select)
   - [Insert with CTEs](#insert-with-ctes)
6. [Select Operations](#select-operations)
   - [Basic Queries](#basic-queries)
   - [Joins](#joins)
   - [ASOF Joins](#asof-joins)
   - [Common Table Expressions (CTEs)](#common-table-expressions-ctes)
   - [Aggregations](#aggregations)
   - [Window Functions](#window-functions)
   - [PIVOT and UNPIVOT](#pivot-and-unpivot)
   - [Set Operations](#set-operations)
7. [Update Operations](#update-operations)
   - [Basic Update](#basic-update)
   - [Update with Subquery](#update-with-subquery)
   - [Update with JOIN (KiUpdate)](#update-with-join-kiupdate)
8. [Delete Operations](#delete-operations)
9. [Table Functions](#table-functions)
   - [FILTER_BY_STRING](#filter_by_string)
   - [EVALUATE_MODEL](#evaluate_model)
10. [Best Practices](#best-practices)
11. [Troubleshooting](#troubleshooting)

---

## Installation and Setup

### Prerequisites

- Python 3.8+
- SQLAlchemy 2.0+
- Kinetica Python API (`gpudb`)

### Installation

```bash
pip install sqlalchemy-kinetica
```

### Required Imports

```python
from sqlalchemy import (
    create_engine, MetaData, Table, Column, Integer, String,
    select, insert, update, delete, func, and_, or_, text,
    cast, case, literal, column, alias, union, intersect, except_
)
from sqlalchemy.sql.ddl import CreateTable

# Kinetica-specific imports
from sqlalchemy_kinetica.dialect import KineticaDialect
from sqlalchemy_kinetica.kinetica_types import (
    TINYINT, SMALLINT, BIGINT, UnsignedBigInteger,
    FLOAT, DOUBLE, REAL, DECIMAL,
    VARCHAR, BLOB, JSON, JSONArray,
    DATE, TIME, DATETIME, TIMESTAMP,
    IPV4, UUID, VECTOR, GEOMETRY, BlobWKT
)
from sqlalchemy_kinetica.custom_commands import (
    ki_insert, KiUpdate, CreateTableAs, Asof, FirstValue,
    PivotSelect, UnpivotSelect, FilterByString, EvaluateModel
)
```

---

## Connection Configuration

### Creating an Engine

```python
from sqlalchemy import create_engine

engine = create_engine(
    "kinetica://",
    connect_args={
        "url": "http://localhost:9191",
        "username": "admin",
        "password": "your_password",
        "default_schema": "my_schema",
        "bypass_ssl_cert_check": True,  # Optional: for self-signed certs
    }
)
```

### Using Environment Variables

```python
import os

ENV_URL = os.getenv("KINETICA_URL", "http://localhost:9191")
ENV_USER = os.getenv("KINETICA_USER", "")
ENV_PASS = os.getenv("KINETICA_PASS", "")
ENV_SCHEMA = os.getenv("KINETICA_SCHEMA", "default")

engine = create_engine(
    "kinetica://",
    connect_args={
        "url": ENV_URL,
        "username": ENV_USER,
        "password": ENV_PASS,
        "default_schema": ENV_SCHEMA,
    }
)
```

### Working with Connections

```python
# Using context manager (recommended)
with engine.connect() as conn:
    result = conn.execute(select(my_table))
    for row in result:
        print(row)

# Using begin() for transaction blocks
with engine.begin() as conn:
    conn.execute(insert(my_table).values(id=1, name="test"))
```

---

## Data Types

### Numeric Types

| Kinetica Type | SQLAlchemy Type | Description |
|---------------|-----------------|-------------|
| `TINYINT` | `TINYINT` | 8-bit signed integer |
| `SMALLINT` | `SMALLINT` | 16-bit signed integer |
| `INTEGER` | `Integer` | 32-bit signed integer |
| `BIGINT` | `BIGINT` | 64-bit signed integer |
| `UNSIGNED BIGINT` | `UnsignedBigInteger` | 64-bit unsigned integer |
| `REAL` | `REAL` | 32-bit floating point |
| `DOUBLE` | `DOUBLE` | 64-bit floating point |
| `FLOAT` | `FLOAT` | Floating point |
| `DECIMAL(p,s)` | `DECIMAL(p,s)` | Fixed-point decimal |

### String Types

| Kinetica Type | SQLAlchemy Type | Description |
|---------------|-----------------|-------------|
| `VARCHAR(n)` | `VARCHAR(n)` | Variable-length string |
| `CHAR(n)` | `CHAR(n)` | Fixed-length string |
| `BLOB` | `BLOB` | Binary large object |
| `JSON` | `JSON` | JSON data |

### Date/Time Types

| Kinetica Type | SQLAlchemy Type | Description |
|---------------|-----------------|-------------|
| `DATE` | `DATE` | Date only |
| `TIME` | `TIME` | Time only |
| `DATETIME` | `DATETIME` | Date and time |
| `TIMESTAMP` | `TIMESTAMP` | Unix timestamp |

### Special Types

| Kinetica Type | SQLAlchemy Type | Description |
|---------------|-----------------|-------------|
| `IPV4` | `IPV4` | IPv4 address |
| `UUID` | `UUID` | Universally unique identifier |
| `VECTOR(n)` | `VECTOR(n)` | Vector of n floats (for ML) |
| `GEOMETRY` | `GEOMETRY` | WKT geometry string |
| `BLOB(WKT)` | `BlobWKT` | WKT binary geometry |
| `ARRAY` | `ARRAY` | Array type |

### Example: Using All Types

```python
from sqlalchemy import Column, Integer, MetaData, Table
from sqlalchemy_kinetica.kinetica_types import *

metadata = MetaData()

all_types_table = Table(
    "various_types",
    metadata,
    Column("i", Integer, primary_key=True),
    Column("ti", TINYINT),
    Column("si", SMALLINT),
    Column("bi", BIGINT),
    Column("ub", UnsignedBigInteger),
    Column("r", REAL),
    Column("d", DOUBLE),
    Column("dc", DECIMAL(10, 4)),
    Column("s", VARCHAR(256)),
    Column("ip", IPV4),
    Column("u", UUID),
    Column("td", DATE),
    Column("tt", TIME),
    Column("dt", DATETIME),
    Column("ts", TIMESTAMP),
    Column("j", JSON),
    Column("bl", BLOB),
    Column("w", BlobWKT),
    Column("g", GEOMETRY),
    Column("v", VECTOR(10)),
    schema="my_schema"
)
```

---

## Table Operations

### Creating Tables

#### Basic Table Creation

```python
from sqlalchemy import MetaData, Table, Column, Integer, String

metadata = MetaData()

employees = Table(
    "employees",
    metadata,
    Column("id", Integer, primary_key=True),
    Column("name", VARCHAR(100)),
    Column("department", VARCHAR(50)),
    Column("salary", DECIMAL(18, 4)),
    schema="my_schema"
)

# Create the table
metadata.create_all(engine)
```

#### Using Table Prefixes

```python
# Replicated table
replicated_table = Table(
    "lookup_data",
    metadata,
    Column("id", Integer, primary_key=True),
    Column("value", VARCHAR(100)),
    schema="my_schema",
    prefixes=["REPLICATED"]
)

# OR REPLACE table
replaceable_table = Table(
    "my_table",
    metadata,
    Column("id", Integer, primary_key=True),
    schema="my_schema",
    prefixes=["OR REPLACE"]
)

# Temporary table
temp_table = Table(
    "temp_data",
    metadata,
    Column("id", Integer, primary_key=True),
    schema="my_schema",
    prefixes=["TEMP"]
)
```

### Table Options and Properties

#### Table Properties

```python
table_properties = {
    "CHUNK SIZE": 1000000,
    "NO_ERROR_IF_EXISTS": "TRUE",
    "TTL": 120,
}

my_table = Table(
    "my_table",
    metadata,
    Column("id", Integer, primary_key=True),
    schema="my_schema",
    info=table_properties  # Pass properties via info parameter
)
```

#### Shard Keys

```python
sharded_table = Table(
    "sharded_data",
    metadata,
    Column("id", Integer, nullable=False, primary_key=True),
    Column("dept_id", Integer, nullable=False, primary_key=True, 
           info={"shard_key": True}),  # Mark as shard key
    Column("data", VARCHAR(100)),
    schema="my_schema"
)
```

#### Indexes

```python
indexed_table = Table(
    "indexed_data",
    metadata,
    Column("id", Integer, primary_key=True),
    Column("dept_id", Integer),
    Column("name", VARCHAR(100)),
    Column("location", GEOMETRY),
    Column("coords_lon", REAL),
    Column("coords_lat", REAL),
    Column("embedding", VECTOR(128)),
    schema="my_schema",
    
    # Standard index (list of lists for multiple indexes)
    kinetica_index=[["dept_id"], ["name"]],
    
    # Chunk skip index (single column name)
    kinetica_chunk_skip_index="id",
    
    # Geospatial indexes
    kinetica_geospatial_index=[
        ["location"],
        ["coords_lon", "coords_lat"]
    ],
    
    # CAGRA index for vector similarity search
    kinetica_cagra_index=["embedding"]
)
```

#### Partitioning

```python
partition_clause = """
PARTITION BY RANGE (YEAR(hire_date))
PARTITIONS
(
    orders_2020 MIN(2020) MAX(2021),
    orders_2021               MAX(2022),
    orders_2022               MAX(2023),
    orders_2023               MAX(2024)
)
"""

partitioned_table = Table(
    "orders",
    metadata,
    Column("id", Integer, primary_key=True),
    Column("hire_date", DATE),
    schema="my_schema",
    kinetica_partition_clause=partition_clause
)
```

#### Tier Strategy

```python
tiered_table = Table(
    "tiered_data",
    metadata,
    Column("id", Integer, primary_key=True),
    schema="my_schema",
    kinetica_tier_strategy="( ( VRAM 1, RAM 7, PERSIST 5 ) )"
)
```

#### Complete Example with All Options

```python
employee = Table(
    "employee",
    metadata,
    Column("id", Integer, nullable=False, primary_key=True),
    Column("dept_id", Integer, nullable=False, primary_key=True, 
           info={"shard_key": True}),
    Column("manager_id", Integer),
    Column("first_name", VARCHAR(30)),
    Column("last_name", VARCHAR(30)),
    Column("sal", DECIMAL(18, 4)),
    Column("hire_date", DATE),
    Column("work_district", BlobWKT),
    Column("office_longitude", REAL),
    Column("office_latitude", REAL),
    Column("profile", VECTOR(10)),
    schema="my_schema",
    prefixes=["OR REPLACE"],
    info={
        "CHUNK SIZE": 1000000,
        "NO_ERROR_IF_EXISTS": "TRUE",
        "TTL": 120
    },
    kinetica_index=[["dept_id"]],
    kinetica_chunk_skip_index="id",
    kinetica_geospatial_index=[
        ["work_district"], 
        ["office_longitude", "office_latitude"]
    ],
    kinetica_cagra_index=["profile"],
    kinetica_tier_strategy="( ( VRAM 1, RAM 7, PERSIST 5 ) )",
    kinetica_partition_clause=partition_clause
)

# Print the DDL
print(CreateTable(employee).compile(engine))

# Create the table
metadata.create_all(engine)
```

### External Tables

```python
external_table = Table(
    "remote_employee",
    metadata,
    schema="my_schema",
    prefixes=["EXTERNAL"],
    info={
        "CHUNK SIZE": 1000000,
        "NO_ERROR_IF_EXISTS": "TRUE",
        "TTL": 120
    },
    kinetica_external_table_remote_query="SELECT * FROM source_db.employees",
    kinetica_external_table_option={
        'DATA SOURCE': 'my_jdbc_datasource',
        'SUBSCRIBE': 'TRUE',
        'REMOTE_QUERY_INCREASING_COLUMN': 'id'
    }
)

# Create the external table
with engine.connect() as conn:
    conn.execute(CreateTable(external_table))
```

### CREATE TABLE AS

```python
from sqlalchemy_kinetica.custom_commands import CreateTableAs

metadata = MetaData()

with engine.connect() as conn:
    source_table = Table('employee', metadata, autoload_with=conn, schema="my_schema")
    
    # Create a new table from a SELECT query
    create_stmt = CreateTableAs(
        '"my_schema"."employee_backup"',
        select(source_table),
        prefixes=["OR REPLACE", "REPLICATED", "TEMP"]
    )
    
    compiled_stmt = create_stmt.compile(conn)
    conn.execute(compiled_stmt)
```

---

## Insert Operations

### Basic Insert

```python
from sqlalchemy import insert

metadata = MetaData()

with engine.connect() as conn:
    employees = Table('employees', metadata, autoload_with=conn, schema="my_schema")
    
    # Single record insert
    stmt = insert(employees).values(
        id=1,
        name="John Doe",
        department="Engineering",
        salary=75000.00
    )
    conn.execute(stmt)
```

### Batch Insert with ki_insert

For bulk inserts, use the `ki_insert` function:

```python
from sqlalchemy_kinetica.custom_commands import ki_insert

metadata = MetaData()

with engine.connect() as conn:
    employees = Table('employees', metadata, autoload_with=conn, schema="my_schema")
    
    # Prepare batch records
    records = [
        {"id": 1, "name": "Anne", "department": "Sales", "salary": 100000},
        {"id": 2, "name": "Bob", "department": "Engineering", "salary": 90000},
        {"id": 3, "name": "Carol", "department": "Marketing", "salary": 85000},
    ]
    
    # Execute batch insert
    conn.execute(ki_insert(employees), records)
```

### Insert with Hints

Kinetica supports special hints for insert operations:

```python
from sqlalchemy_kinetica.custom_commands import ki_insert

# Upsert mode - update existing records with matching primary key
insert_stmt = ki_insert(employees, insert_hint="KI_HINT_UPDATE_ON_EXISTING_PK")

records = [
    {"id": 1, "name": "Anne Updated", "department": "Sales", "salary": 110000},
]

conn.execute(insert_stmt, records)
```

### Insert from Select

```python
from sqlalchemy import select

metadata = MetaData()

with engine.connect() as conn:
    source = Table('employees', metadata, autoload_with=conn, schema="my_schema")
    target = Table('employees_backup', metadata, autoload_with=conn, schema="my_schema")
    
    # Insert from SELECT
    insert_stmt = ki_insert(target).from_select(
        ["id", "name", "department", "salary"],
        select(source.c.id, source.c.name, source.c.department, source.c.salary)
        .where(source.c.salary > 50000)
    )
    
    conn.execute(insert_stmt)
```

### Insert with CTEs

```python
metadata = MetaData()

with engine.connect() as conn:
    employee = Table('employee', metadata, autoload_with=conn, schema="my_schema")
    target = Table('dept2_emp_mgr_roster', metadata, autoload_with=conn, schema="my_schema")
    
    # Define CTEs
    dept2_emp = (
        select(employee.c.first_name, employee.c.last_name, employee.c.manager_id)
        .where(employee.c.dept_id == 2)
        .cte(name='dept2_emp')
    )
    
    dept2_mgr = (
        select(employee.c.first_name, employee.c.last_name, employee.c.id)
        .where(employee.c.dept_id == 2)
        .cte(name='dept2_mgr')
    )
    
    # Join CTEs
    select_stmt = (
        select(
            dept2_emp.c.first_name.label('emp_first_name'),
            dept2_emp.c.last_name.label('emp_last_name'),
            dept2_mgr.c.first_name.label('mgr_first_name'),
            dept2_mgr.c.last_name.label('mgr_last_name')
        )
        .select_from(
            dept2_emp.join(dept2_mgr, dept2_emp.c.manager_id == dept2_mgr.c.id)
        )
    )
    
    # Insert with CTE
    insert_stmt = target.insert().from_select(
        ['emp_first_name', 'emp_last_name', 'mgr_first_name', 'mgr_last_name'],
        select_stmt
    )
    
    # Compile with literal_binds for CTE support
    compiled = insert_stmt.compile(conn, compile_kwargs={"literal_binds": True})
    conn.execute(compiled)
```

---

## Select Operations

### Basic Queries

```python
from sqlalchemy import select, func

metadata = MetaData()

with engine.connect() as conn:
    employees = Table('employees', metadata, autoload_with=conn, schema="my_schema")
    
    # Simple select all
    query = select(employees)
    result = conn.execute(query)
    
    # Select specific columns
    query = select(employees.c.name, employees.c.salary)
    
    # With WHERE clause
    query = select(employees).where(employees.c.salary > 50000)
    
    # With ORDER BY
    query = select(employees).order_by(employees.c.salary.desc())
    
    # With LIMIT
    query = select(employees).limit(10)
    
    # With OFFSET
    query = select(employees).limit(10).offset(20)
```

### Joins

#### Inner Join (Infixed)

```python
metadata = MetaData()

with engine.connect() as conn:
    employee = Table('employee', metadata, autoload_with=conn, schema="my_schema")
    
    # Self-join with aliases
    e = employee.alias("e")
    m = employee.alias("m")
    
    query = select(
        (e.c.last_name + ', ' + e.c.first_name).label("Employee_Name"),
        (m.c.last_name + ', ' + m.c.first_name).label("Manager_Name")
    ).select_from(
        e.join(m, e.c.manager_id == m.c.id)
    )
    
    result = conn.execute(query)
```

#### Left Outer Join

```python
from sqlalchemy import nullsfirst, asc

query = select(
    e.c.last_name,
    m.c.last_name.label("manager_last_name")
).select_from(
    e.outerjoin(m, e.c.manager_id == m.c.id)
).where(
    e.c.dept_id.in_([1, 2, 3])
).order_by(
    nullsfirst(asc(m.c.id)),
    e.c.hire_date
)

result = conn.execute(query)
```

### ASOF Joins

ASOF joins are used for time-series data to match rows based on temporal proximity:

```python
from sqlalchemy_kinetica.custom_commands import Asof

metadata = MetaData()

with engine.connect() as conn:
    quotes = Table('quotes', metadata, autoload_with=conn, schema="my_schema").alias("q")
    trades = Table('trades', metadata, autoload_with=conn, schema="my_schema").alias("t")
    
    # Define the ASOF condition
    asof_condition = Asof(
        trades.c.dt,           # Left column (trade datetime)
        quotes.c.open_dt,      # Right column (quote datetime)
        text("INTERVAL '-1' DAY"),  # Relative range begin
        text("INTERVAL '0' DAY"),   # Relative range end
        text("MAX")            # MIN or MAX for tie-breaking
    )
    
    # Build the query
    query = select(
        trades.c.id,
        trades.c.dt.label('execution_dt'),
        quotes.c.open_dt.label('quote_dt'),
        trades.c.price.label('execution_price'),
        quotes.c.open_price
    ).select_from(
        trades.outerjoin(
            quotes, 
            (trades.c.ticker == quotes.c.symbol) & asof_condition
        )
    )
    
    # Compile with literal_binds
    compiled = query.compile(conn, compile_kwargs={"literal_binds": True})
    result = conn.execute(compiled)
```

### Common Table Expressions (CTEs)

```python
metadata = MetaData()

with engine.connect() as conn:
    employee = Table('employee', metadata, autoload_with=conn, schema="my_schema")
    
    # Define the CTE
    dept_salary_cte = (
        select(
            employee.c.manager_id,
            employee.c.sal.label('salary')
        )
        .where(employee.c.dept_id == 2)
        .cte(name='dept2_emp_sal_by_mgr')
    )
    
    # Use the CTE in main query
    query = select(
        dept_salary_cte.c.manager_id.label('mgr_id'),
        func.max(dept_salary_cte.c.salary).label('max_salary'),
        func.count().label('emp_count')
    ).group_by(
        dept_salary_cte.c.manager_id
    )
    
    compiled = query.compile(conn)
    result = conn.execute(compiled)
```

### Aggregations

#### Without GROUP BY

```python
query = select(
    func.round(func.avg(employees.c.salary), 2).label('avg_salary'),
    func.count().label('total_count'),
    func.min(employees.c.salary).label('min_salary'),
    func.max(employees.c.salary).label('max_salary')
)

# Compile with literal_binds for numeric arguments
compiled = query.compile(conn, compile_kwargs={"literal_binds": True})
result = conn.execute(compiled)
```

#### With GROUP BY

```python
query = select(
    employees.c.department,
    func.count().label('emp_count'),
    func.avg(employees.c.salary).label('avg_salary')
).group_by(
    employees.c.department
).having(
    func.count() > 5
).order_by(
    employees.c.department
)

compiled = query.compile(conn, compile_kwargs={"literal_binds": True})
result = conn.execute(compiled)
```

#### ROLLUP

```python
from sqlalchemy import case

query = select(
    case(
        (func.grouping(employees.c.department) == 1, '<ALL DEPARTMENTS>'),
        else_=func.nvl(employees.c.department, '<UNKNOWN>')
    ).label('dept_group'),
    func.avg(employees.c.salary).label('avg_salary')
).group_by(
    func.rollup(employees.c.department)
)

compiled = query.compile(conn, compile_kwargs={"literal_binds": True})
result = conn.execute(compiled)
```

### Window Functions

#### RANK and PERCENT_RANK

```python
from sqlalchemy import cast

metadata = MetaData()

with engine.connect() as conn:
    nyctaxi = Table('nyctaxi', metadata, autoload_with=conn, schema='demo')
    
    # Rank function
    ranked_fare = func.rank().over(
        partition_by=nyctaxi.c.vendor_id,
        order_by=nyctaxi.c.total_amount
    ).label('ranked_fare')
    
    # Percent rank (multiply by 100)
    percent_ranked = func.percent_rank().over(
        partition_by=nyctaxi.c.vendor_id,
        order_by=nyctaxi.c.total_amount
    ) * 100
    
    query = select(
        nyctaxi.c.vendor_id,
        nyctaxi.c.total_amount.label('fare'),
        ranked_fare,
        cast(percent_ranked, FLOAT).label('percent_ranked_fare')
    ).where(
        nyctaxi.c.passenger_count == 3
    )
    
    # Must compile with literal_binds for arithmetic with literals
    compiled = query.compile(conn, compile_kwargs={"literal_binds": True})
    result = conn.execute(compiled)
```

#### FIRST_VALUE with IGNORE NULLS

```python
from sqlalchemy_kinetica.custom_commands import FirstValue
from sqlalchemy import desc

tip_amount = nyctaxi.c.tip_amount

# FIRST_VALUE with IGNORE NULLS
lowest_tip = tip_amount - FirstValue(tip_amount, ignore_nulls=True).over(
    partition_by=nyctaxi.c.vendor_id,
    order_by=tip_amount
)

highest_tip = tip_amount - FirstValue(tip_amount, ignore_nulls=True).over(
    partition_by=nyctaxi.c.vendor_id,
    order_by=desc(tip_amount)
)

query = select(
    nyctaxi.c.vendor_id,
    tip_amount,
    lowest_tip.label('diff_from_lowest'),
    highest_tip.label('diff_from_highest')
)

compiled = query.compile(conn, compile_kwargs={"literal_binds": True})
result = conn.execute(compiled)
```

#### NTILE

```python
# Subquery with NTILE
quartile_subquery = select(
    nyctaxi.c.vendor_id,
    nyctaxi.c.total_amount,
    func.ntile(4).over(
        partition_by=nyctaxi.c.vendor_id,
        order_by=nyctaxi.c.total_amount
    ).label('quartile')
).subquery()

# Main query using interquartile range
query = select(
    quartile_subquery.c.vendor_id,
    func.decimal(func.avg(quartile_subquery.c.total_amount)).label('avg_amount'),
    func.decimal(
        func.avg(
            case(
                (quartile_subquery.c.quartile.in_([2, 3]), 
                 quartile_subquery.c.total_amount),
                else_=None
            )
        )
    ).label('avg_interquartile_amount')
).group_by(
    quartile_subquery.c.vendor_id
)

compiled = query.compile(conn, compile_kwargs={"literal_binds": True})
result = conn.execute(compiled)
```

### PIVOT and UNPIVOT

#### PIVOT

```python
from sqlalchemy_kinetica.custom_commands import PivotSelect
from sqlalchemy import column

metadata = MetaData()

with engine.connect() as conn:
    phone_list = Table('phone_list', metadata, autoload_with=conn, schema="my_schema")
    
    # Create PIVOT query
    query = (
        PivotSelect(
            column("name"),
            column("Home_Phone"),
            column("Work_Phone"),
            column("Cell_Phone")
        )
        .select_from(phone_list)
        .pivot(
            "max(phone_number) AS Phone",  # Aggregate function
            "phone_type",                   # Pivot column
            ["'Home'", "'Work'", "'Cell'"]  # Pivot values
        )
        .order_by(column("name"))
    )
    
    compiled = query.compile(conn)
    result = conn.execute(compiled)
```

#### UNPIVOT

```python
from sqlalchemy_kinetica.custom_commands import UnpivotSelect
from sqlalchemy import column

metadata = MetaData()

with engine.connect() as conn:
    customer_contact = Table('customer_contact', metadata, 
                            autoload_with=conn, schema="my_schema").alias("cc")
    
    # Create subquery with renamed columns
    subquery = select(
        customer_contact.c.name,
        customer_contact.c.home_phone.label('Home'),
        customer_contact.c.work_phone.label('Work'),
        customer_contact.c.cell_phone.label('Cell')
    )
    
    # Create UNPIVOT query
    query = (
        UnpivotSelect(
            column("name"),
            column("phone_type"),
            column("phone_number")
        )
        .select_from(subquery)
        .unpivot(
            "phone_number",           # Value column
            "phone_type",             # Type column
            ["Home", "Work", "Cell"]  # Columns to unpivot
        )
        .order_by(column("name"), column("phone_type"))
    )
    
    compiled = query.compile(conn)
    result = conn.execute(compiled)
```

### Set Operations

#### UNION

```python
from sqlalchemy import union

metadata = MetaData()

with engine.connect() as conn:
    lunch = Table('lunch_menu', metadata, autoload_with=conn, schema="my_schema")
    dinner = Table('dinner_menu', metadata, autoload_with=conn, schema="my_schema")
    
    # UNION (removes duplicates)
    union_query = union(
        select(lunch.c.item, lunch.c.price),
        select(dinner.c.item, dinner.c.price)
    )
    
    compiled = union_query.compile(conn)
    result = conn.execute(compiled)
```

#### UNION ALL

```python
from sqlalchemy import union_all

union_all_query = union_all(
    select(lunch.c.item, lunch.c.price),
    select(dinner.c.item, dinner.c.price)
)

compiled = union_all_query.compile(conn)
result = conn.execute(compiled)
```

#### INTERSECT

```python
from sqlalchemy import intersect

intersect_query = intersect(
    select(lunch.c.item, lunch.c.price),
    select(dinner.c.item, dinner.c.price)
)

compiled = intersect_query.compile(conn)
result = conn.execute(compiled)
```

#### EXCEPT

```python
from sqlalchemy import except_

except_query = except_(
    select(lunch.c.item, lunch.c.price),
    select(dinner.c.item, dinner.c.price)
)

compiled = except_query.compile(conn)
result = conn.execute(compiled)
```

---

## Update Operations

### Basic Update

```python
from sqlalchemy import update

metadata = MetaData()

with engine.connect() as conn:
    employees = Table('employees', metadata, autoload_with=conn, schema="my_schema")
    
    # Simple update
    stmt = update(employees).where(
        employees.c.id == 5
    ).values(
        salary=75000,
        department="Sales"
    )
    
    conn.execute(stmt)
```

### Update with Subquery

```python
from sqlalchemy import alias, func

metadata = MetaData()

with engine.connect() as conn:
    e_lookup = Table('employee', metadata, autoload_with=conn, schema="my_schema")
    e_base = alias(e_lookup, name="b")
    
    # Correlated subquery
    max_sal_in_dept = (
        select(func.max(e_lookup.c.sal))
        .where(e_base.c.dept_id == e_lookup.c.dept_id)
    ).scalar_subquery()
    
    # Update with subquery in SET
    stmt = update(e_base).values(
        sal=max_sal_in_dept * literal(0.1) + e_base.c.sal * literal(0.9)
    )
    
    compiled = stmt.compile(conn, compile_kwargs={"literal_binds": True})
    conn.execute(compiled)
```

### Update with JOIN (KiUpdate)

Kinetica supports UPDATE with FROM/JOIN clauses. Use `KiUpdate` for this:

#### Infixed JOIN Syntax

```python
from sqlalchemy_kinetica.custom_commands import KiUpdate

metadata = MetaData()

with engine.connect() as conn:
    employee = Table('employee', metadata, autoload_with=conn, schema="my_schema")
    employee_backup = Table('employee_backup', metadata, autoload_with=conn, schema="my_schema")
    
    # UPDATE with JOIN
    update_stmt = KiUpdate(
        employee_backup,
        from_table=employee,
        join_condition=(employee_backup.c.id == employee.c.id)
    ).values(
        sal=employee.c.sal,
        manager_id=employee.c.manager_id
    )
    
    compiled = update_stmt.compile(conn)
    conn.execute(compiled)
```

Generated SQL:
```sql
UPDATE employee_backup SET sal=employee.sal, manager_id=employee.manager_id
FROM employee_backup
JOIN employee ON employee_backup.id = employee.id
```

#### Non-Infixed JOIN Syntax

```python
# UPDATE with FROM and WHERE (non-infixed join)
update_stmt = KiUpdate(
    employee_backup,
    from_table=employee,
    where_condition=(employee_backup.c.id == employee.c.id)
).values(
    sal=employee.c.sal,
    manager_id=employee.c.manager_id
)

compiled = update_stmt.compile(conn)
conn.execute(compiled)
```

Generated SQL:
```sql
UPDATE employee_backup SET sal=employee.sal, manager_id=employee.manager_id
FROM employee_backup, employee
WHERE employee_backup.id = employee.id
```

---

## Delete Operations

### Basic Delete

```python
from sqlalchemy import delete

metadata = MetaData()

with engine.connect() as conn:
    employees = Table('employees', metadata, autoload_with=conn, schema="my_schema")
    
    # Delete specific records
    stmt = delete(employees).where(employees.c.id == 5)
    conn.execute(stmt)
    
    # Delete with multiple conditions
    stmt = delete(employees).where(
        and_(
            employees.c.department == "Sales",
            employees.c.salary < 30000
        )
    )
    conn.execute(stmt)
```

### Delete with Subquery

```python
from sqlalchemy import delete, func

metadata = MetaData()

with engine.connect() as conn:
    e_base = Table('employee', metadata, autoload_with=conn, schema="my_schema").alias("b")
    e_lookup = Table('employee', metadata, autoload_with=conn, schema="my_schema").alias("l")
    
    # Correlated subquery
    max_id_in_dept = (
        select(func.max(e_lookup.c.id))
        .where(e_base.c.dept_id == e_lookup.c.dept_id)
    ).scalar_subquery()
    
    # Delete using subquery
    delete_stmt = delete(e_base).where(e_base.c.id == max_id_in_dept)
    
    compiled = delete_stmt.compile(conn)
    conn.execute(compiled)
```

---

## Table Functions

### FILTER_BY_STRING

Filter records based on string pattern matching:

```python
from sqlalchemy_kinetica.custom_commands import FilterByString
from sqlalchemy import column, text

metadata = MetaData()

with engine.connect() as conn:
    event_log = Table('event_log', metadata, autoload_with=conn, schema="my_schema")
    
    # SELECT mode - returns results
    filter_func = FilterByString(
        table_name=select(column('event_time'), column('message'))
                   .select_from(text(event_log.fullname)),
        column_names='message',
        mode='contains',
        expression='ERROR'
    )
    
    compiled = filter_func.compile(conn, compile_kwargs={"literal_binds": True})
    result = conn.execute(compiled)
```

Available modes:
- `contains` - Match if column contains expression
- `equals` - Exact match
- `search` - Full-text search (cannot be used with column_names)
- `regex` - Regular expression match

### EVALUATE_MODEL

Execute ML model evaluation:

```python
from sqlalchemy_kinetica.custom_commands import EvaluateModel
from sqlalchemy import column, text

metadata = MetaData()

with engine.connect() as conn:
    # Source table selection
    source = select(column("raster_uri")).select_from(
        text(f"{schema}.raster_input")
    ).compile(conn)
    
    # SELECT mode - returns results directly
    stmt = EvaluateModel(
        model='my_model',
        deployment_mode='batch',
        replications=1,
        source_table=source
    )
    
    compiled = stmt.compile(conn, compile_kwargs={"literal_binds": True})
    result = conn.execute(compiled)
    
    # EXECUTE mode - writes to destination table
    stmt = EvaluateModel(
        model='my_model',
        deployment_mode='batch',
        replications=1,
        source_table=source,
        destination_table=f"{schema}.model_output"  # Triggers EXECUTE mode
    )
    
    compiled = stmt.compile(conn, compile_kwargs={"literal_binds": True})
    conn.execute(compiled)
```

---

## Best Practices

### 1. Use `literal_binds` for Complex Queries

When queries contain arithmetic operations with numeric literals or CTEs, compile with `literal_binds`:

```python
# Good - prevents <UNKNOWN> type errors
compiled = query.compile(conn, compile_kwargs={"literal_binds": True})
result = conn.execute(compiled)

# Also good - use literal() for inline values
from sqlalchemy import literal
query = select(employees.c.salary * literal(1.1))
```

### 2. Use `ki_insert` for Batch Operations

For bulk inserts, always use `ki_insert` instead of standard insert:

```python
# Good - optimized for batch inserts
conn.execute(ki_insert(employees), list_of_records)

# Less efficient for large batches
for record in list_of_records:
    conn.execute(insert(employees).values(**record))
```

### 3. Autoload Tables for Existing Schemas

When working with existing tables, use `autoload_with`:

```python
# Good - gets accurate schema from database
employees = Table('employees', metadata, autoload_with=conn, schema="my_schema")

# Avoids schema mismatches
```

### 4. Use Proper Table Prefixes

Leverage Kinetica-specific table types:

```python
# Replicated tables for small lookup data
lookup = Table("codes", metadata, prefixes=["REPLICATED"], ...)

# Temporary tables for intermediate results
temp = Table("staging", metadata, prefixes=["TEMP"], ...)

# OR REPLACE for idempotent deployments
Table("my_table", metadata, prefixes=["OR REPLACE"], ...)
```

### 5. Index Strategy

- Use `kinetica_chunk_skip_index` on primary key columns for efficient chunk pruning
- Use `kinetica_index` on frequently filtered columns
- Use `kinetica_geospatial_index` for geometry columns used in spatial queries
- Use `kinetica_cagra_index` for vector similarity search

---

## Troubleshooting

### Error: `Cannot apply '*' to arguments of type '<DOUBLE> * <UNKNOWN>'`

**Cause:** Numeric literals in arithmetic expressions are sent as parameters without type information.

**Solution:** Compile with `literal_binds=True`:

```python
compiled = query.compile(conn, compile_kwargs={"literal_binds": True})
result = conn.execute(compiled)
```

Or use `literal()`:

```python
from sqlalchemy import literal
query = select(column * literal(100))
```

### Error: `No literal value renderer is available for literal value`

**Cause:** Using `literal_binds=True` on INSERT/UPDATE with values that don't have type information.

**Solution:** Don't use `literal_binds=True` for basic INSERT/UPDATE operations:

```python
# Wrong
stmt = employees.update().values(salary=70000)
conn.execute(stmt.compile(conn, compile_kwargs={"literal_binds": True}))

# Right
stmt = employees.update().values(salary=70000)
conn.execute(stmt.compile(conn))
```

### Error: `AttributeError: 'PivotSelect' object has no attribute 'kwargs'`

**Cause:** Using an older version of the dialect that incorrectly used `kwargs` for Select subclasses.

**Solution:** Update to the latest version of the Kinetica SQLAlchemy dialect.

### Error: `NoSuchModuleError: Can't load plugin: sqlalchemy.dialects`

**Cause:** Custom kwargs keys don't follow the `{dialect}_{option}` format.

**Solution:** Ensure all custom kwargs use the `kinetica_` prefix.

### Connection Issues

**SSL Certificate Errors:**
```python
engine = create_engine("kinetica://", connect_args={
    "url": "https://your-server:9191",
    "bypass_ssl_cert_check": True,  # For self-signed certs
    ...
})
```

**Timeout Issues:**
```python
engine = create_engine("kinetica://", connect_args={
    "url": "http://your-server:9191",
    "timeout": 120,  # Increase timeout in seconds
    ...
})
```

---

## Appendix: Quick Reference

### Custom Commands

| Class | Purpose |
|-------|---------|
| `ki_insert()` | Batch insert with hints |
| `KiUpdate` | UPDATE with JOIN support |
| `CreateTableAs` | CREATE TABLE AS SELECT |
| `Asof` | ASOF join function |
| `FirstValue` | FIRST_VALUE with IGNORE NULLS |
| `PivotSelect` | SELECT with PIVOT |
| `UnpivotSelect` | SELECT with UNPIVOT |
| `FilterByString` | FILTER_BY_STRING table function |
| `EvaluateModel` | EVALUATE_MODEL for ML |

### Table Kwargs

| Kwarg | Purpose |
|-------|---------|
| `kinetica_index` | Create standard indexes |
| `kinetica_chunk_skip_index` | Create chunk skip index |
| `kinetica_geospatial_index` | Create geospatial indexes |
| `kinetica_cagra_index` | Create CAGRA vector index |
| `kinetica_tier_strategy` | Set tier strategy |
| `kinetica_partition_clause` | Define partitioning |
| `kinetica_external_table_remote_query` | External table remote query |
| `kinetica_external_table_file_paths` | External table file paths |
| `kinetica_external_table_option` | External table options |

### Column Info Options

| Info Key | Purpose |
|----------|---------|
| `shard_key` | Mark column as shard key |
| `TEXT_SEARCH` | Enable text search |
| `DICT` | Enable dictionary encoding |
| `INIT_WITH_UUID` | Auto-generate UUID |
| `INIT_WITH_NOW` | Auto-set to current timestamp |

---
